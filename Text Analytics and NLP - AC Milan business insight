#############################################################################
###### Text Analytics and Natural Language Processing (NLP) #################
###### Business Insight Report on AC Milan ##################################
###### Lorenzo Donati #######################################################
#############################################################################

###STEP 1: Getting access to AC Milan Tweets

install.packages("rtweet")
library(rtweet)

consumer_key <- "xxxxxx"
consumer_secret <- "xxxxxx"
access_token <- "xxxxxx"
access_secret <- "xxxxxx"
name_of_app <- "xxxxxx"

twitter_token <- create_token(
  app = name_of_app,
  consumer_key = consumer_key,
  consumer_secret = consumer_secret,
  access_token = access_token,
  access_secret = access_secret)

acm <- search_tweets("#ACMilan", n = 1000, lang= "en", include_rts = FALSE)

###STEP 2: Putting the dataset in a data frame

install.packages("dplyr")
library(dplyr)
acm_df <- data.frame(line=1:993, text=acm$text)
print(acm)

###STEP 3: tokenizing the acm_df dataframe

install.packages("tidyverse")
install.packages("tidytext")
library(tidyverse)
library(tidytext)

acm_token_list <- acm_df %>%
  unnest_tokens(word, text)

library(stringr)
data(stop_words)

frequencies_acm_tokens_nostop <- acm_df %>%
  unnest_tokens(word, text) %>%
  anti_join(stop_words) %>% #here's where we remove tokens
  count(word, sort=TRUE)

print(frequencies_acm_tokens_nostop)

# Create a list of custom stopwords that should be added
word <- c("t.co", "ac", "https", "milan", "acmilan", "sempremilan", "forzamilan")
lexicon <-  rep("custom", times=length(word))

# Create a dataframe from the two vectors above
mystopwords <- data.frame(word, lexicon)
names(mystopwords) <- c("word", "lexicon")

# Add the dataframe to stop_words df that exists in the library stopwords
stop_words <-  dplyr::bind_rows(stop_words, mystopwords)
View(stop_words)

new_frequencies_acm_tokens_nostop <- acm_df %>%
  unnest_tokens(word, text) %>%
  anti_join(stop_words) %>% #here's where we remove tokens
  count(word, sort=TRUE)

print(new_frequencies_acm_tokens_nostop)

### STEP 4:  token frequency histograms

library(ggplot2)
freq_hist <- acm_df %>%
  unnest_tokens(word,text) %>%
  anti_join(stop_words) %>%
  count(word, sort=TRUE) %>%
  filter(n>54) %>% # we need this to eliminate all the low count words
  mutate(word=reorder(word, n)) %>%
  ggplot(aes(word, n))+
  geom_col()+
  xlab(NULL)+
  coord_flip()
print(freq_hist)


#SENTIMENT ANALYSIS - negative/positive words

library(janeaustenr)
library(tidytext)
library(tidyr)
library(tidytuesdayR)

nrcsurprise <- get_sentiments("nrc") %>%
  filter(sentiment == "surprise")

#inner joining the ACMilan and the surprise sentiments

new_frequencies_acm_tokens_nostop %>%
  inner_join(nrcsurprise) %>%
  count(word, sort=T)

# Comparing different sentiment libraries on ACMilan

afinn <- new_frequencies_acm_tokens_nostop %>%
  inner_join(get_sentiments("afinn"))%>%
  summarise(sentiment=sum(value)) %>%
  mutate(method="AFINN")

bing_and_nrc <- bind_rows(
  new_frequencies_acm_tokens_nostop%>%
    inner_join(get_sentiments("bing"))%>%
    mutate(method = "Bing et al."),
  new_frequencies_acm_tokens_nostop %>%
    inner_join(get_sentiments("nrc") %>%
                 filter(sentiment %in% c("positive", "negative"))) %>%
    mutate(method = "NRC")) %>%
  count(method,  sentiment) %>%
  spread(sentiment, n, fill=0) %>%
  mutate(sentiment = positive-negative)

library(ggplot2)
bind_rows(afinn, bing_and_nrc) %>%
  ggplot(aes(method, sentiment, fill=method))+
  geom_col(show.legend=FALSE)+
  facet_wrap(~method, ncol =1, scales= "free_y")

### Bing - Most common positive and negative words 

acm_bing <- new_frequencies_acm_tokens_nostop %>%
  inner_join(get_sentiments("bing")) %>%
  arrange(desc(n))

acm_bing

acm_bing %>%
  group_by(sentiment) %>%
  top_n(10, n) %>%
  ungroup() %>%
  mutate(word=reorder(word, n)) %>%
  ggplot(aes(word, n, fill=sentiment)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~sentiment, scales = "free_y")+
  labs(y="Contribution to sentiment", x=NULL)+
  coord_flip()

#N-GRAMS ANALYSIS - to see most common bigrams

library(dplyr)
library(tidytext)
library(tidyr)
library(tidytuesdayR)

acm_bigrams <- acm_df %>%
  unnest_tokens(bigram, text, token = "ngrams", n=2)

acm_bigrams #We want to see the bigrams (words that appear together, "pairs")

acm_bigrams %>%
  count(bigram, sort = TRUE) #this has many stop words, need to remove them 

#to remove stop words from the bigram data, we need to use the separate function:
library(tidyr)

bigrams_separated <- acm_bigrams %>%
  separate(bigram, c("word1", "word2"), sep = " ")

#remove stopwords
bigrams_filtered <- bigrams_separated %>%
  filter(!word1 %in% stop_words$word) %>%
  filter(!word2 %in% stop_words$word)

#creating the new bigram, "no-stop-words":
acm_bigram_counts <- bigrams_filtered %>%
  count(word1, word2, sort = TRUE)
#want to see the new bigrams
acm_bigram_counts

### Visualizing a Bigram Network

#install.packages("igraph")
library(igraph)
bigram_graph <- acm_bigram_counts %>%
  filter(n>7) %>%
  graph_from_data_frame()

bigram_graph

#install.packages("ggraph")
library(ggraph)
library(ggplot2)

ggraph(bigram_graph, layout = "fr") +
  geom_edge_link()+
  geom_node_point()+
  geom_node_text(aes(label=name), vjust =1, hjust=1)
